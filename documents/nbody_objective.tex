% This file is part of the Streams project.
% Copyright 2012 David W. Hogg (NYU).
% All rights reserved.

\documentclass[12pt,pdftex,preprint]{aastex}
\usepackage{amssymb,amsmath,mathrsfs}

\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\given}{\,|\,}
\newcommand{\bg}{\mathrm{bg}}
\newcommand{\dyn}{\mathrm{dyn}}
\newcommand{\ic}{\mathrm{I.C.}}
\newcommand{\normal}{\mathscr{N}}

\begin{document}

\title{A likelihood function for n-body models and point data:\\
       The Palomar 5 Stream}
\author{some combination of David W. Hogg, APW, AJ, DFM, others}

\begin{abstract}
In a wide range of astrophysical contexts, the dominant theoretical
methods are n-body simulations or methods that produce samplings of a
density field or other point sets.  In many contexts, the dominant
observational data are also points in some space: stars on the sky or
in phase space, galaxies in redshift space, and the like.  In
principle, model testing ought to proceed by comparing likelihoods or
marginalized likelihoods or posterior probabilities.  Here we make a
very general proposal for constructing a likelihood---a probability
for the data given the output of the n-body simulation.  It treats the
data as being generated by a mixture model built from the n-body
points.  We employ this model---mixed with a flexible background
model---to fit a numerical model of a tidal stream to observed stars
in the vicinity of Palomar 5 Stream.  We find that the Milky Way is
XXX and the Stream is YYY.
\end{abstract}

\section{Introduction}

...Example problems where the theory and the data are both point sets in
an observable space.

...Law papers.  Binney and Eyre papers.

...Paper that is very close but fails to marginalize
(\citealt{varghese}).

...Paper that is correct but works by instantiating an artificial and
unnecessary binning underlying the point sets (\citealt{saha}).

...General statement of the problem.  Include backgrounds/foregrounds
in the data statement.

...Missing data.  Data from heterogeneous sources.

...Selection functions.  Short answer is apply same fn to model as
happened to data.  If you don't know it, things get hard; will return
to this in the discussion section.

\section{Likelihood generalities}

You have measured in a survey of some kind $N$ data points $X_n$ in a
$d$-dimensional observable space $X$.  The data points could be stars
and the space could be 6-dimensional phase space.  It could be
galaxies in 3-dimensional redshift space.  It could be quasars in just
2-dimensional celestial coordinates.  Or it could be stars in a much
higher dimensional space of phase space plus chemical indicators.

...Notes here about why we should work in the observed space and not
phase-space.  Cite issues with parallax and distance infecting
everything...

Along with each measurement comes a positive-definite $d\times d$
variance tensor $\sigma^2_n$, or a non-negative-definite inverse
variance tensor $\sigma^{-2}_n$.  This is a measure of the uncertainty
in the space, and also at the same time takes account of \emph{missing
  data}: Dimensions or directions which are unmeasured in the $X$
space for point $X_n$ are dimensions or directions with zero
eigenvalue in $\sigma^{-2}_n$.  That is, we can mix together stars
that have measured positions and velocities in with stars that just
have measured angular positions and with stars that have measured
proper motions; each of these different kinds of star has a different
rank to the $\sigma^{-2}_n$ tensor.

At the same time, you have a theory or model that can be computed with
a ``n-body'' simulation that generates $K$ points $x_k$ in that same
space.  The theory has a large vector or blob of parameters $\theta$.
If the theory is interesting and predictive, the positions $x_k$
depend non-trivially on the parameters $\theta$.

The fundamental idea is that if these model points are representative,
the data can be thought of as being \emph{generated by} the model
points.  Model point $x_k$ can generate a data point $X_n$ with
probability $p(X_n\given x_k,\theta,I)$; this would be the
single-data-point likelihood of the parameters and this one model
point $x_k$.  Because (usually) the identity $k$ of the model point
explaining data point $n$ is not of scientific interest, we want to
\emph{marginalize out} $k$ to get a probability $p(X_n\given\theta,I)$
that is independent of $k$.

The math for all this is
\begin{eqnarray}
p(X_n\given x_k,\theta,I) &=& \normal(X_n\given x_k,\sigma^2_n+\Sigma_k^2)
\\
p(X_n\given\theta,I) &=& \sum_{k=1}^K P_k\,p(X_n\given x_k,\theta)
\\
\theta &\equiv& \setof{\theta_\dyn, \theta_\ic, \setof{P_k, \Sigma_k^2}_{k=1}^K }
\\
I &\equiv& \setof{K, \setof{\sigma^2_n}_{n=1}^N, \cdots}
\\
1 &=& \sum_{k=1}^K P_ky
\quad ,
\end{eqnarray}
where $I$ represents our inviolate prior information about the problem
(the number of points we are generating with the theory and the noise
model), $\normal(x\given m, V)$ is the $d$-dimensional Gaussian
distribution for $x$ given mean $m$ and variance tensor $V$,
$\Sigma^2_k$ is a smoothing variance tensor in the $X$ space to
account for the fact that the model points are not to be treated as
infinitesimal, $\theta$ can be divided into dynamical parameters
(Milky Way potential parameters, for example) and initial-condition
parameters (the orbit and phase of the body of Palomar 5, for example)
and the smoothing, and the likelihood can be marginalized over model
points $k$ by summing weighted by the prior probabilities $P_k$.

In the (very common) case that all the model points are
equally weighed or equally representative or predictive for the data,
\begin{eqnarray}
P_k &=& \frac{1}{K}
\\
\Sigma^2_k &=& \Sigma^2
\quad ,
\end{eqnarray}
where $\Sigma^2$ is a common squared-width (covariance matrix in the
$d$-dimensional data space) for every model point.  Alternatively, the
$\Sigma^2_k$ can be set adaptively using the local density of model
points, in the style of adaptive kernel density estimation (for
example, \citealt{adaptiveKDE1}, \citealt{adaptiveKDE2}).  From here
on for specificity we will treat all the $\Sigma^2_k$ to be identical
and drop the $k$ subscript.

HOGG... Now introduce a background model...
\begin{eqnarray}
P(\setof{X_n}_{n=1}^N\given\theta,I) &=& \prod_{n=1}^N p(X_n\given\theta,I)
\\
p(X_n\given\theta,I) &=& P_\bg\,p(X_n\given \bg,\theta) + \sum_{k=1}^K P_k\,p(X_n\given x_k,\theta)
\\
1 &=& P_\bg + \sum_{k=1}^K P_k
\\
\theta &\equiv& \setof{\theta_\dyn, \theta_\ic, \Sigma^2, \setof{P_k}_{k=1}^K, \theta_\bg, P_\bg}
\quad .
\end{eqnarray}
Again, if all the model points are equally weighted,
\begin{eqnarray}
P_k &=& \frac{[1 - P_\bg]}{K}
\quad .
\end{eqnarray}

\section{Palomar 5 specifics}

...Kuepper model.

...Geha data

\section{Results and discussion}

...Results

...Discussion

...What happens if the selection fn is super insane or unknown?  This
happens surprisingly often.

\acknowledgements It is a pleasure to thank
  Brendon Brewer (Auckland)
for helpful comments.  The authors were partially supported by NASA
(grant NNX12AI50G) and NSF (IIS-1124794).  This research made use of
the NASA \textsl{Astrophysics Data System}.

\begin{thebibliography}{}
\bibitem[Loftsgaarden \& Quesenberry(1965)]{adaptiveKDE1}
  Loftsgaarden,~D.~O. \& Quesenberry,~C.~P.\ 1965, Ann.\ Math.\ Statist.\ 36, 1049
\bibitem[Saha(1998)]{saha}
  Saha, P.\ 1998, \aj, 115, 1206 
\bibitem[Terrell \& Scott(1992)]{adaptiveKDE2}
  Terrell,~G.~R. \& Scott,~D.~W.\ 1992, Ann.\ Statist.\ 20, 1236
\bibitem[Varghese et al.(2011)]{varghese}
  Varghese, A., Ibata, R., \& Lewis, G.~F.\ 2011, \mnras, 417, 198 
\end{thebibliography}

\end{document}
