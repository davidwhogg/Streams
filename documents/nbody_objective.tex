% This file is part of the Streams project.
% Copyright 2012 David W. Hogg (NYU).
% All rights reserved.

% to-do
% -----
% - make sure it makes clear from the introduction onwards, that the data space can include chemical abundances!!
% - write introduction
% - get Kuepper on board

\documentclass[12pt,pdftex,preprint]{aastex}
\usepackage{amssymb,amsmath,mathrsfs}

\newcommand{\documentname}{\textsl{Article}}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\given}{\,|\,}
\newcommand{\bg}{\mathrm{bg}}
\newcommand{\dyn}{\mathrm{dyn}}
\newcommand{\ic}{\mathrm{I.C.}}
\newcommand{\normal}{\mathscr{N}}

\begin{document}

\title{A likelihood function for n-body models and point data:\\
       The Palomar 5 Stream}
\author{some combination of David W. Hogg, Andreas Kuepper (we hope), APW, AJ, DFM, others}

\begin{abstract}
In a wide range of astrophysical contexts, the dominant theoretical
methods are n-body simulations or methods that produce samplings of a
density field or other point sets.  In many contexts, the dominant
observational data are also points in some space: stars on the sky or
in phase space, galaxies in redshift space, and the like.  In
principle, model testing ought to proceed by comparing likelihoods or
marginalized likelihoods or posterior probabilities.  Here we make a
very general proposal for constructing a likelihood---a probability
for the data given the output of the n-body simulation.  It treats the
data as being generated by a mixture model built from the n-body
points.  We employ this model---mixed with a flexible background
model---to fit a numerical model of a tidal stream to observed stars
in the vicinity of Palomar 5 Stream.  We find that the Milky Way is
XXX and the Stream is YYY.
\end{abstract}

\section{Introduction}

In many domains in astrophysics, the theory is computed through n-body
or particle or pseudo-particle simulations.  This includes large-scale
cosmology, galaxy formation, molecular cloud and star formation,
debris disks, and many other systems.  Even when the theoretical
computations are not n-body, it is often easiest to transmit and
manipulate their outputs using point samplings of the complex density
fields they produce.  It is also the case that in many domains of
interest, the data are measurements of point particles, like stars in
our Galaxy and galaxies in the large-scale structure of our Universe.
Again, even when the observations aren't point-like, it can be useful
to represent measured density fields with a sampling.  Thus arises
frequently situations in empirical astrophysics in which distributions
of data points need to be compared to distributions of theory points
for the purposes of model fitting, parameter estimation, model
comparison, experimental design, and prediction.  The best methods for
these tasks---the methods that properly transmit information and
uncertainty from the data to the inferences and outputs---involve
probabilistic modeling, in which there is a likelihood function (a
probability for the data given the parameters) and prior probabilities
(quantitative plausibilities) for (at least) the nuisance parameters.
In this \documentname, we are going to propose a general likelihood
function for problems of this type.

For specificity, we will focus on tidal streams of stars observed in
the Milky Way, and within that, the cold Palomar 5 stream (cite).
This stream is a very coherent, informative structure in the Milky
Way; we would like to be able to use it to measure the dynamical
properties of the Galaxy, but of course it's detailed morphology in
position--velocity space (phase space) depends on a combination of the
initial conditions and internal structure of the disrupting globular
cluster that generates the stream and also Milky Way parameters.  Our
goal of obtaining precise Milky Way parameters in the face of
globular-cluster nuisance parameters motivated our search for proper
probabilistic methods.  In this \documentname\ we will demonstrate the
use of our proposed likelihood function with this system, and learn
something (we hope) about the Milky Way and Palomar 5 along the way.

...Law papers.  Binney and Eyre papers...

...Paper that is very close but fails to marginalize
(\citealt{varghese})...

...Paper that is correct but works by instantiating an artificial and
unnecessary binning underlying the point sets (\citealt{saha})...

...General statement of the problem.  Include backgrounds/foregrounds
in the data statement...

A key property of any probabilistically justified method is that it
can deal with arbitrarily \emph{heterogeneous data}:
There are always particular data points for which some dimensions
(radial velocity, for example) has not been measured, and in a typical
astronomical experiment, no two objects have the same noise or error
or uncertainty properties.
Missing data and heterogenous noise properties are one and the same
from our point of view; ``missing data'' are just directions in the
data space where the noise variance is arbitrarily large (or the
inverse variance vanishes).
A properly specified likelihood function is a pdf for the data given
the model parameters; it is a representation of the noise model.
As such, it can (or must) account for the individual heterogenous
noise variances for every dimension of every data point.

Another key property of any proper method is that it can deal with
realistic \emph{selection functions}.
Data are only measured within certain bondaries in the data space.
The model may generate predictions or model points or n-body
particles outside the selection boundaries.
If you know precisely this selection function---even if it does
not have sharp boundaries but rather regions in which the
probability of including a data point in the sample declines from
unity to zero gradually---then in a properly specified probabilistic
model you can apply the selection function identically to the model
points as it has been applied (by the experimenter or the hardware)
to the data points.

There are, of course, many observational projects in which the
selection function is not precisely known, or cannot be described
as a function applied independently to each data point.
When objects have been selected for observation using external
information that was subsequently lost or over-written, or when
objects have been selected in groups based on group properties
(for example if the top hundred stars are chosen in each square
degree), then although it is not \emph{impossible} to build an
accurate likelihood function, it does become much more difficult:
Either the properties of the selection function must be promoted
to the status of fittable model parameters, or else the full-data
likelihood cannot be represented as a product of single-point
likelihoods, or both.  We will return to some of these issues in
the discussion.

\section{Likelihood generalities}

You have measured in a survey of some kind $N$ data points $X_n$ in a
$d$-dimensional observable space $X$.  The data points could be stars
and the space could be 6-dimensional phase space.  It could be
galaxies in 3-dimensional redshift space.  It could be quasars in just
2-dimensional celestial coordinates.  Or it could be stars in a much
higher dimensional space of phase space plus chemical indicators.

...Notes here about why we should work in the observed space and not
phase-space.  Cite issues with parallax and distance infecting
everything...

Along with each measurement comes a positive-definite $d\times d$
variance tensor $\sigma^2_n$, or a non-negative-definite inverse
variance tensor $\sigma^{-2}_n$.  This is a measure of the uncertainty
in the space, and also at the same time takes account of \emph{missing
  data}: Dimensions or directions which are unmeasured in the $X$
space for point $X_n$ are dimensions or directions with zero
eigenvalue in $\sigma^{-2}_n$.  That is, we can mix together stars
that have measured positions and velocities in with stars that just
have measured angular positions and with stars that have measured
proper motions; each of these different kinds of star has a different
rank to the $\sigma^{-2}_n$ tensor.

At the same time, you have a theory or model that can be computed with
a ``n-body'' simulation that generates $K$ points $x_k$ in that same
space.  The theory has a large vector or blob of parameters $\theta$.
If the theory is interesting and predictive, the positions $x_k$
depend non-trivially on the parameters $\theta$.

The fundamental idea is that if these model points are representative,
the data can be thought of as being \emph{generated by} the model
points.  Model point $x_k$ can generate a data point $X_n$ with
probability $p(X_n\given x_k,\theta,I)$; this would be the
single-data-point likelihood of the parameters and this one model
point $x_k$.  Because (usually) the identity $k$ of the model point
explaining data point $n$ is not of scientific interest, we want to
\emph{marginalize out} $k$ to get a probability $p(X_n\given\theta,I)$
that is independent of $k$.

The math for all this is
\begin{eqnarray}
p(X_n\given x_k,\theta,I) &=& \normal(X_n\given x_k,\sigma^2_n+\Sigma_k^2)
\\
p(X_n\given\theta,I) &=& \sum_{k=1}^K P_k\,p(X_n\given x_k,\theta)
\\
\theta &\equiv& \setof{\theta_\dyn, \theta_\ic, \setof{P_k, \Sigma_k^2}_{k=1}^K }
\\
I &\equiv& \setof{K, \setof{\sigma^2_n}_{n=1}^N, \cdots}
\\
1 &=& \sum_{k=1}^K P_ky
\quad ,
\end{eqnarray}
where $I$ represents our inviolate prior information about the problem
(the number of points we are generating with the theory and the noise
model), $\normal(x\given m, V)$ is the $d$-dimensional Gaussian
distribution for $x$ given mean $m$ and variance tensor $V$,
$\Sigma^2_k$ is a smoothing variance tensor in the $X$ space to
account for the fact that the model points are not to be treated as
infinitesimal, $\theta$ can be divided into dynamical parameters
(Milky Way potential parameters, for example) and initial-condition
parameters (the orbit and phase of the body of Palomar 5, for example)
and the smoothing, and the likelihood can be marginalized over model
points $k$ by summing weighted by the prior probabilities $P_k$.

In the (very common) case that all the model points are
equally weighed or equally representative or predictive for the data,
\begin{eqnarray}
P_k &=& \frac{1}{K}
\\
\Sigma^2_k &=& \Sigma^2
\quad ,
\end{eqnarray}
where $\Sigma^2$ is a common squared-width (covariance matrix in the
$d$-dimensional data space) for every model point.  Alternatively, the
$\Sigma^2_k$ can be set adaptively using the local density of model
points, in the style of adaptive kernel density estimation (for
example, \citealt{adaptiveKDE1}, \citealt{adaptiveKDE2}).  From here
on for specificity we will treat all the $\Sigma^2_k$ to be identical
and drop the $k$ subscript.

In real situations of data analysis, it is often the case that some of
the data points are drawn not from the structures or distributions of
interest, but rather from some background or foreground population
that is not of much scientific interest.  Often in data analysis
attempts are made to hard-identify these background points and clip
them out, but in many cases---and when it comes to tidal
streams---this is not possible with good fidelity.  The only
probabilistically righteous approach is to build a probabilistic model
of the uninteresting background and fit it simultaneously with the
structures of interest.  In this approach, the data are treated as
being drawn from a \emph{mixture model}, a superposition of target
(interesting) and background (uninteresting) distributions.  Mixture
models are a good idea even when background membership \emph{can} be
done fairly well, in part because it includes these decisions
probabilistically in the model fitting and in part because results can
be obtained effectively \emph{marginalizing out} all such decisions.

The mixture model math looks like this:
\begin{eqnarray}
P(\setof{X_n}_{n=1}^N\given\theta,I) &=& \prod_{n=1}^N p(X_n\given\theta,I)
\\
p(X_n\given\theta,I) &=& P_\bg\,p(X_n\given \bg,\theta) + \sum_{k=1}^K P_k\,p(X_n\given x_k,\theta)
\\
1 &=& P_\bg + \sum_{k=1}^K P_k
\\
\theta &\equiv& \setof{\theta_\dyn, \theta_\ic, \Sigma^2, \setof{P_k}_{k=1}^K, \theta_\bg, P_\bg}
\quad ,
\end{eqnarray}
where $K$-point mixture of theory kernels is mixed with the background
distribution, the background distribution has parameters
$\setof{\theta_\bg, P_\bg}$, and the background probability $P_\bg$
becomes an additional prior probability.  In this case, however, the
prior $P_\bg$ is going to be a fit model parameter, so rather than
thinking of it as a prior, it is better to think of it as a
hyperparameter.  Again, if all the model points are equally weighted,
\begin{eqnarray}
P_k &=& \frac{[1 - P_\bg]}{K}
\quad .
\end{eqnarray}

\section{Palomar 5 specifics}

...Kuepper model.

...Geha data

\section{Results and discussion}

...Results

...Discussion

...What happens if the selection fn is super insane or unknown?  This
happens surprisingly often.

\acknowledgements It is a pleasure to thank
  Brendon Brewer (Auckland)
for helpful comments.  The authors were partially supported by NASA
(grant NNX12AI50G) and NSF (IIS-1124794).  This research made use of
the NASA \textsl{Astrophysics Data System}.

\begin{thebibliography}{}
\bibitem[Loftsgaarden \& Quesenberry(1965)]{adaptiveKDE1}
  Loftsgaarden,~D.~O. \& Quesenberry,~C.~P.\ 1965, Ann.\ Math.\ Statist.\ 36, 1049
\bibitem[Saha(1998)]{saha}
  Saha, P.\ 1998, \aj, 115, 1206 
\bibitem[Terrell \& Scott(1992)]{adaptiveKDE2}
  Terrell,~G.~R. \& Scott,~D.~W.\ 1992, Ann.\ Statist.\ 20, 1236
\bibitem[Varghese et al.(2011)]{varghese}
  Varghese, A., Ibata, R., \& Lewis, G.~F.\ 2011, \mnras, 417, 198 
\end{thebibliography}

\end{document}
