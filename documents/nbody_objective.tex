% This file is part of the Streams project.
% Copyright 2012, 2013 David W. Hogg (NYU).
% All rights reserved.

% to-do
% -----
% - make sure it makes clear from the introduction onwards, that the data space can include chemical abundances!!
% - write introduction

\documentclass[12pt,pdftex,preprint]{aastex}
\usepackage{amssymb,amsmath,mathrsfs}

\newcommand{\documentname}{\textsl{Article}}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\given}{\,|\,}
\newcommand{\bg}{\mathrm{bg}}
\newcommand{\dyn}{\mathrm{dyn}}
\newcommand{\ic}{\mathrm{I.C.}}
\newcommand{\normal}{\mathscr{N}}

\begin{document}

\title{A likelihood function for n-body models and point data:\\
       The Palomar 5 Stream}
\author{many people, including David~W.~Hogg}

\begin{abstract}
In a wide range of astrophysical contexts, the dominant theoretical
methods are n-body simulations or methods that produce samplings of a
density field or other point sets.  In many contexts, the dominant
observational data are also points in some space: stars on the sky or
in phase space, galaxies in redshift space, and the like.  In
principle, model testing ought to proceed by comparing likelihoods or
marginalized likelihoods or posterior probabilities.  Here we make a
very general proposal for constructing a likelihood---a probability
for the data given the output of the n-body simulation.  It treats the
data as being generated by a mixture model built from the n-body
points.  We employ this model---mixed with a flexible background
model---to fit a numerical model of a tidal stream to observed stars
in the vicinity of Palomar 5 Stream.  We find that the Milky Way is
XXX and the Stream is YYY.
\end{abstract}

\section{Introduction}

In many domains in astrophysics, the theory is computed through n-body
or particle or pseudo-particle simulations.  This includes large-scale
cosmology, galaxy formation, molecular cloud and star formation,
debris disks, and many other systems.  Even when the theoretical
computations are not n-body, it is often easiest to transmit and
manipulate their outputs using point samplings of the complex density
fields they produce.  It is also the case that in many domains of
interest, the data are measurements of point particles, like stars in
our Galaxy and galaxies in the large-scale structure of our Universe.
Again, even when the observations aren't point-like, it can be useful
to represent measured density fields with a sampling.  Thus arises
frequently situations in empirical astrophysics in which distributions
of data points need to be compared to distributions of theory points
for the purposes of model fitting, parameter estimation, model
comparison, experimental design, and prediction.  The best methods for
these tasks---the methods that properly transmit information and
uncertainty from the data to the inferences and outputs---involve
probabilistic modeling, in which there is a likelihood function (a
probability for the data given the parameters) and prior probabilities
(quantitative plausibilities) for (at least) the nuisance parameters.
In this \documentname, we are going to propose a general likelihood
function for problems of this type.

For specificity, we will focus on tidal streams of stars observed in
the Milky Way, and within that, the cold Palomar 5 stream (cite).
This stream is a very coherent, informative structure in the Milky
Way; we would like to be able to use it to measure the dynamical
properties of the Galaxy, but of course it's detailed morphology in
position--velocity space (phase space) depends on a combination of the
initial conditions and internal structure of the disrupting globular
cluster that generates the stream and also Milky Way parameters.  Our
goal of obtaining precise Milky Way parameters in the face of
globular-cluster nuisance parameters motivated our search for proper
probabilistic methods.  In this \documentname\ we will demonstrate the
use of our proposed likelihood function with this system, and learn
something (we hope) about the Milky Way and Palomar 5 along the way.

...Law papers.  Binney and Eyre papers...

...Paper that is very close but fails to marginalize
(\citealt{varghese})...

...Paper that is correct but works by instantiating an artificial and
unnecessary binning underlying the point sets (\citealt{saha})...

The general statement of the problem is as follows: You have measured
$N$ points $X_n$ in a high-dimensional observation space $X$.  Each
measured data point has its own noise variance tensor $\sigma^2_n$.
You have a theoretical model with (a possibly very high dimensional)
parameter blob $\theta$; this model generates $K$ points
$x_k(\theta)$.  In addition, you expect that some (unknown) fraction
of your data points are drawn not from the structure of interest but
instead from some uninteresting ``background'' structure.  The
question is, can we write down a justifiable likelihood---probability
for the $X_n$---as a function of the model parameters $\theta$?  In
what follows we will show that we can, and that it is practicable and
useful.

A key property of any probabilistically justified method is that it
can deal with arbitrarily \emph{heterogeneous data}:
There are always particular data points for which some dimensions
(radial velocity, for example) has not been measured, and in a typical
astronomical experiment, no two objects have the same noise or error
or uncertainty properties.
Missing data and heterogenous noise properties are one and the same
from our point of view; ``missing data'' are just directions in the
data space where the noise variance is arbitrarily large (or the
inverse variance vanishes).
A properly specified likelihood function is a pdf for the data given
the model parameters; it is a representation of the noise model.
As such, it can (or must) account for the individual heterogenous
noise variances for every dimension of every data point.

Another key property of any proper method is that it can deal with
realistic \emph{selection functions}.
Data are only measured within certain bondaries in the data space.
The model may generate predictions or model points or n-body
particles outside the selection boundaries.
If you know precisely this selection function---even if it does
not have sharp boundaries but rather regions in which the
probability of including a data point in the sample declines from
unity to zero gradually---then in a properly specified probabilistic
model you can apply the selection function identically to the model
points as it has been applied (by the experimenter or the hardware)
to the data points.

There are, of course, many observational projects in which the
selection function is not precisely known, or cannot be described
as a function applied independently to each data point.
When objects have been selected for observation using external
information that was subsequently lost or over-written, or when
objects have been selected in groups based on group properties
(for example if the top hundred stars are chosen in each square
degree), then although it is not \emph{impossible} to build an
accurate likelihood function, it does become much more difficult:
Either the properties of the selection function must be promoted
to the status of fittable model parameters, or else the full-data
likelihood cannot be represented as a product of single-point
likelihoods, or both.  We will return to some of these issues in
the discussion.

In what follows, we will assume that the data points (vectors) $X_n$
come with known uncertainty variances (tensors, possibly diagonal)
$\sigma^2_n$.  In some cases of interest in astronomy (and many cases
\emph{outside} of astronomy), the uncertainty variances are not known.
There is nothing in principle challenging with making these
uncertainty tensors (or their eigenvectors) model parameters.  We
won't consider the general case of this, although we will include in
our model a smoothing tensor $\Sigma$ that accounts for the smoothness
of the model but can also hide or account for under-estimation of
observational uncertainties.  In practice, if the data are expected to
be heteroskedastic, and you don't know \emph{any} of the uncertainty
variances $\sigma^2_n$, inference can get challenging; usually strong
priors on the $\sigma^2_n$ are required for good performance.  In a
separate project, we are looking at this question, with the priors
themselves learned from the data (Richards et al, forthcoming).

Finally but importantly, the space $X$ of observables can be
\emph{any} space of observables, provided that the observations come
associated with approximately Gaussian uncertainties with (at least
approximately) known uncertainty variances.  In our example problem,
we will consider the $X$ space to be kinematic phase space (position
and velocity) but it could just as easily include some chemical
abundance space, or stellar parameter space (temperature, log $g$,
rotation, and so on).  This makes the method here extremely general;
our own goals are to use it for modeling the joint chemical and
dynamical state of the Milky Way, but it has an enormous range of uses
in cosmology and Galactic astrophysics.

\section{Likelihood generalities}

You have measured (in an observation or experiment or survey of some
kind) $N$ data points $X_n$ in a $d$-dimensional observable space $X$.
In our example (below), the data points will be stars and the space
will be the kinematic observables of celestial position, distance,
proper motion, and radial velocity.  As we have noted (above), the
space could be any space, including any observable measured with
quasi-normal uncertainty.

Along with each measurement comes a positive-definite $d\times d$
variance tensor $\sigma^2_n$, or a non-negative-definite inverse
variance tensor $\sigma^{-2}_n$.  This is a measure of the uncertainty
in the space, and also at the same time takes account of \emph{missing
  data}: Dimensions or directions which are unmeasured in the $X$
space for point $X_n$ are dimensions or directions with zero
eigenvalue in $\sigma^{-2}_n$.  That is, we can mix together stars
that have measured positions and velocities in with stars that just
have measured angular positions and with stars that have measured
proper motions; each of these different kinds of star has a different
rank to the $\sigma^{-2}_n$ tensor.

One restriction we ought to apply to the space $X$ is that it ought to
be the space \emph{in which the observations are made} or a trivial
transformation (rotation, shift, or affine) thereof.  The reason is
that if the observational uncertainties are Gaussian or close to
Gaussian in the observed space, they will \emph{not} be close to
Gaussian in a radical transformation of that space.  For example, in
our example (below), the observables of log-distance (distance
modulus) and proper motion are usually converted straightforwardly to
velocity.  Because this transformation involves exponentiation and
multiplication of observables, it leads to a multi-dimensional and
non-Gaussian uncertainty even if the original observables have
independent Gaussian uncertainties.  The case is even worse when it is
parallaxes that are observed, because there is a division of
observables, with the denominator close to zero.

The theory or model is computed with or represented in an ``n-body''
representation that includes $K$ points $x_k$ in that same $X$ space
as the data points $X_n$.  The theory has a large vector or blob of
parameters $\theta$.  If the theory is interesting and predictive, the
positions $x_k$ depend non-trivially on the parameters $\theta$.

The fundamental idea is that if these model points are representative,
the data can be thought of as being \emph{generated by} the model
points.  Model point $x_k$ (which depends on the parameters $\theta$)
can generate a data point $X_n$ with probability $p(X_n\given
k,\theta,I)$; this would be the single-data-point likelihood of the
parameters and this one model point $x_k$.  Because (usually) the
identity $k$ of the model point explaining data point $n$ is not of
scientific interest, we want to \emph{marginalize out} $k$ to get a
probability $p(X_n\given\theta,I)$ that is independent of $k$.

The math for all this is
\begin{eqnarray}
p(X_n\given k,\theta,I) &=& \normal(X_n\given x_k,\sigma^2_n+\Sigma_k^2)
\\
p(X_n\given\theta,I) &=& \sum_{k=1}^K P_k\,p(X_n\given k,\theta,I)
\\
\theta &\equiv& \setof{\theta_\dyn, \theta_\ic, \setof{P_k, \Sigma_k^2}_{k=1}^K }
\\
I &\equiv& \setof{K, \setof{\sigma^2_n}_{n=1}^N, \cdots}
\\
1 &=& \sum_{k=1}^K P_k
\quad ,
\end{eqnarray}
where $I$ represents our inviolate prior information about the problem
(the number of points we are generating with the theory and the noise
model), $\normal(x\given m, V)$ is the $d$-dimensional Gaussian
distribution for $x$ given mean $m$ and variance tensor $V$,
$\Sigma^2_k$ is a smoothing variance tensor in the $X$ space to
account for the fact that the model points are not to be treated as
infinitesimal, $\theta$ can be divided into dynamical parameters
(Milky Way potential parameters, for example) and initial-condition
parameters (the orbit and phase of the body of Palomar 5, for example)
and the smoothing, and the likelihood can be marginalized over model
points $k$ by summing weighted by the prior probabilities $P_k$.

In the (very common) case that all the model points are
equally weighed or equally representative or predictive for the data,
\begin{eqnarray}
P_k &=& \frac{1}{K}
\\
\Sigma^2_k &=& \Sigma^2
\quad ,
\end{eqnarray}
where $\Sigma^2$ is a common squared-width (covariance matrix in the
$d$-dimensional data space) for every model point.  Alternatively, the
$\Sigma^2_k$ can be set adaptively using the local density of model
points, in the style of adaptive kernel density estimation (for
example, \citealt{adaptiveKDE1}, \citealt{adaptiveKDE2}).  From here
on for specificity we will treat all the $\Sigma^2_k$ to be identical
and drop the $k$ subscript.

In real situations of data analysis, it is often the case that some of
the data points are drawn not from the structures or distributions of
interest, but rather from some background or foreground population
that is not of much scientific interest.  Often in data analysis
attempts are made to hard-identify these background points and clip
them out, but in many cases---and when it comes to tidal
streams---this is not possible with good fidelity.  The only
probabilistically righteous approach is to build a probabilistic model
of the uninteresting background and fit it simultaneously with the
structures of interest.  In this approach, the data are treated as
being drawn from a \emph{mixture model}, a superposition of target
(interesting) and background (uninteresting) distributions.  Mixture
models are a good idea even when background membership \emph{can} be
done fairly well, in part because it includes these decisions
probabilistically in the model fitting and in part because results can
be obtained effectively \emph{marginalizing out} all such decisions.

The mixture model math looks like this:
\begin{eqnarray}
P(\setof{X_n}_{n=1}^N\given\theta,I) &=& \prod_{n=1}^N p(X_n\given\theta,I)
\\
p(X_n\given\theta,I) &=& P_\bg\,p(X_n\given \bg,\theta) + \sum_{k=1}^K P_k\,p(X_n\given k,\theta,I)
\\
1 &=& P_\bg + \sum_{k=1}^K P_k
\\
\theta &\equiv& \setof{\theta_\dyn, \theta_\ic, \Sigma^2, \setof{P_k}_{k=1}^K, \theta_\bg, P_\bg}
\quad ,
\end{eqnarray}
where $K$-point mixture of theory kernels is mixed with the background
distribution, the background distribution has parameters
$\setof{\theta_\bg, P_\bg}$, and the background probability $P_\bg$
becomes an additional prior probability.  In this case, however, the
prior $P_\bg$ is going to be a fit model parameter, so rather than
thinking of it as a prior, it is better to think of it as a
hyperparameter.  Again, if all the model points are equally weighted,
\begin{eqnarray}
P_k &=& \frac{[1 - P_\bg]}{K}
\quad .
\end{eqnarray}

...Marginalization and the necessity of priors

...Choices for priors

\section{Palomar 5 specifics}

...Kuepper model.

...SDSS imaging catalog data (plus cuts or whatever)

...Geha data

\section{Results and discussion}

...Results

...Discussion

...What happens if the selection fn is super insane or unknown?  This
happens surprisingly often.

\acknowledgements It is a pleasure to thank
  Brendon Brewer (Auckland)
for helpful comments.  The authors were partially supported by NASA
(grant NNX12AI50G) and NSF (IIS-1124794).  This research made use of
the NASA \textsl{Astrophysics Data System}.

\begin{thebibliography}{}
\bibitem[Loftsgaarden \& Quesenberry(1965)]{adaptiveKDE1}
  Loftsgaarden,~D.~O. \& Quesenberry,~C.~P.\ 1965, Ann.\ Math.\ Statist.\ 36, 1049
\bibitem[Saha(1998)]{saha}
  Saha, P.\ 1998, \aj, 115, 1206 
\bibitem[Terrell \& Scott(1992)]{adaptiveKDE2}
  Terrell,~G.~R. \& Scott,~D.~W.\ 1992, Ann.\ Statist.\ 20, 1236
\bibitem[Varghese et al.(2011)]{varghese}
  Varghese, A., Ibata, R., \& Lewis, G.~F.\ 2011, \mnras, 417, 198 
\end{thebibliography}

\end{document}
